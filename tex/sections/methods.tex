\begin{lemma}
For every continuous curve $y : [t_0, t_1] \mapsto S^2$ in the sphere, there is a vector $L : [t_0, t_1] \mapsto \R^3$ such that $y(t)$ is the solution to the ordinary differential equation 
\begin{equation}
    \frac{dx}{dt}(t) = L(t) \times x(t) \qquad x(t_0) = y(t_0)
\end{equation}
\end{lemma}

\begin{proof}
Consider $L(t) = \alpha(t) x(t) + x(t) \times \dot x (t)$ for any function $\alpha : \R \mapsto \R$ function. 
It is easy to check that this satisfies the differential equations. 
Any value of $\alpha$ will work, meaning that there are many $L(t)$ that will satisfy the differential equation.
However, notice that $\alpha \equiv 0$ is the one that makes $L(t)$ to have minimal Euclidean norm. 
\end{proof}

This previous result give us a general formula to parametrized curves in the sphere. 
Given observations $y_1, y_2, \ldots, y_N$, we can perform non-parametric regression using \textit{trajectory matching} \cite{ramsay2017dynamic} by optimizing the loss function 
\begin{equation}
 \min_{L(t), x_0} \,
 \sum_{i=1}^N \| y_i - \ODESolve (t_i ; x_0, L(t)) \|_2^2
\end{equation}
where $\ODESolve (t_i ; x_0, L(t))$ is the numerical solution of the differential equation \eqref{eq:sphere-ode} with initial condition $x(t_0) = x_0$ and time-dependent angular momentum $L(t)$. 
In order to solve this optimization problem, we are going to parametrize the function $L: [t_0, t_N] \mapsto \R^3$ with a small neural network with unknown weights and biases. 
Any other regressor with good expresivity that satisfies that the output is differentiable with respect to its arguments and parameters will serves as well. 

In order to solve the previous problem, we can directly discretize in time, compute the gradient and treat this as a trend filter problem. It is possible that gradients can be computed manually, although the automatic differentiation machinery could work perfectly well here.
Another option is to directly calculate the continuous adjoint equation and solve it to compute the derivative of the first term. 

Notice that we may also want to include a penalization term of the form $\| L(t) \|_2^2$ in order to penalize rotations with large angular velocity, which we know are not physically feasible. 


\subsection{Regularization}

Depending the structure we want to impose in the path, we may be interested in imposing different types of regularization or constraints in the curves we parametrization in the sphere. 
Expressing curves in $S^2$ as solutions of the differential equations \eqref{eq:sphere-ode} where $L(t)$ is a time-dependent function allow us to directly regularize $L(t)$ instead of the numerical solution $x(t)$. 
Since $L(t)$ is parametrized at the same time by a neural network or other regressor, this can be easily differentiated, a requirement for the gradient-based optimization procedure.
In this work we are going to consider regularizations of the form 
\begin{equation}
    \Reg_k^p (L(\cdot))
    = 
    \int_{t_0}^{t_1} \left \| \frac{d^k L}{dt^k} (\tau) \right \|_p^p d\tau
\end{equation}
with $k$ the order derivative ($k=0$ for no derivative) and $p$ the type of penallization ($p=1$ for Lasso, $p=2$ for Ridge).
Examples of these form of regularization include
\begin{enumerate}
    \item \textbf{Small angular velocities.} As we described in our previous result, there are many instant rotations that give rise to the same differential path. If we are interested in finding the rotation with minimum angular velocity, we can add a the penalty
    \begin{equation}
        \Reg_0^2 (L(\cdot)) = \int_{t_0}^{t_1} \| L(\theta) \|_2^2 d\tau
    \end{equation}
    \item \textbf{Smooth trajectories.}
    \begin{equation}
        \Reg_1^2 (L(\cdot)) = \int_{t_0}^{t_1} \left \| \frac{dL}{dt}(\tau) \right \|_2^2 d\tau
    \end{equation}
    \item \textbf{Piecewise dynamics. } It is usally assumed that APWPs in paleomagnetism consist on a sequence of solid rotations that stay stable over certain periods of time. Since constant values of $L(t)$ are associated to constant rotations with fixed rotation axis and angular velocity, we can impose sparse changes in the values of $L(t)$ by a Lasso penalty in the first derivatives of $L(t)$:
    \begin{equation}
        \Reg_1^1 (L(\cdot)) = \int_{t_0}^{t_1} \left \| \frac{dL}{dt}(\tau) \right \|_1 d\tau
    \end{equation}
    This is the continuous version of the trend filtering problem \cite{tibs-trend-filter}.
\end{enumerate}
Notice that we have used the unified notation 

\subsection{Incorporating time uncertainties}

\begin{equation}
 \min_{L(t), x_0, \delta} \,
 \sum_{i=1}^N \| y_i - \ODESolve (t_i + \delta_i; x_0, L(t)) \|_2^2
 + 
 \lambda_1 
 \Reg (L(t))
 + 
 \lambda_2 
 h(\| \delta \|_2)
\end{equation}
where $h:\R \mapsto \R$ is an Huber function.

\subsection{Optimize and discretize vs Discretize and optimize}

